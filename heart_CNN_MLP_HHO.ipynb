{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "resident-equipment",
   "metadata": {},
   "source": [
    "# About the Project:\n",
    "\n",
    "In this project, we will classify a Heart Disease Dataset to predict people who have the disease.\n",
    "The models used for this task are MLP and CNN that applied with Keras; also, we used a metaheuristic algorithm(HHO) to optimize the number of nodes in each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uahmiFTBwcX8",
   "metadata": {
    "id": "uahmiFTBwcX8"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "WHO announced that cardiovascular diseases is the top one killer over the world. There are seventeen million people died from it every year, especially heart disease. Prevention is better than cure. If we can evaluate the risk of every patient who probably has heart disease, that is, not only patients but also everyone can do something earlier to keep illness away.\n",
    "\n",
    "This dataset is a real data including important features of patients. This time we will build the predictable model by XGBoost library. Before predict the test dataset, use concept of crossvalid to find the optimised parameters. after that, the model can calculate the weight of each features, so we can easily understand which feature is more influent than others.\n",
    "\n",
    "Confusion matrix is a common technique to figure out the accuracy of the model. From the standpoint of medicine, the recall rate is more important than precision rate because no one want to be misdiagnosed if the one actually have heart disease. So we will check the recall performance. After that, roc curve can help us evaluate the model, and then we'll explore the features if the model is good enough.\n",
    "\n",
    "\"Shap\" is a powerful library to know whether each feature is positive or negative relationship with heart disease. At the end of the report, we double confirm the result, and we can also know which feature affect each patient the most."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "v0NW57v8wcYH",
   "metadata": {
    "id": "v0NW57v8wcYH"
   },
   "source": [
    "## Exploratory Analysis\n",
    "\n",
    "### There are thirteen features and one target as below:\n",
    "\n",
    "* age: The person's age in years\n",
    "\n",
    "* sex: The person's sex (1 = male, 0 = female)\n",
    "\n",
    "* cp: The chest pain experienced (Value 1: typical angina, Value 2: atypical angina, Value 3: non-anginal pain, Value 4: asymptomatic)\n",
    "\n",
    "* trestbps: The person's resting blood pressure (mm Hg on admission to the hospital)\n",
    "\n",
    "* chol: The person's cholesterol measurement in mg/dl\n",
    "\n",
    "* fbs: The person's fasting blood sugar (> 120 mg/dl, 1 = true; 0 = false)\n",
    "\n",
    "* restecg: Resting electrocardiographic measurement (0 = normal, 1 = having ST-T wave abnormality, 2 = showing probable or definite left ventricular hypertrophy by Estes' criteria)\n",
    "\n",
    "* thalach: The person's maximum heart rate achieved\n",
    "\n",
    "* exang: Exercise induced angina (1 = yes; 0 = no)\n",
    "\n",
    "* oldpeak: ST depression induced by exercise relative to rest\n",
    "\n",
    "* slope: the slope of the peak exercise ST segment (Value 1: upsloping, Value 2: flat, Value 3: downsloping)\n",
    "\n",
    "* ca: The number of major vessels (0-3)\n",
    "\n",
    "* thal: A blood disorder called thalassemia (3 = normal; 6 = fixed defect; 7 = reversable defect)\n",
    "\n",
    "* target: Heart disease (0 = no, 1 = yes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Cdvp6fMkwcYJ",
   "metadata": {
    "id": "Cdvp6fMkwcYJ"
   },
   "source": [
    "# Importing Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3Dy1G1WXw3F6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3Dy1G1WXw3F6",
    "outputId": "bf3cbce4-9039-4780-9182-7581c648ec0b"
   },
   "outputs": [],
   "source": [
    "pip install pygad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-Sk8yb6qwcYK",
   "metadata": {
    "id": "-Sk8yb6qwcYK"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import pygad.kerasga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FYHKtb44wcYM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "FYHKtb44wcYM",
    "outputId": "5249ade1-64fd-4a9c-9b72-e3646aadf353",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./heart.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "otSlCsoawcYP",
   "metadata": {
    "id": "otSlCsoawcYP"
   },
   "outputs": [],
   "source": [
    "X = df.drop(\"target\", axis=1)\n",
    "y = df[\"target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fZRMFbrwcYQ",
   "metadata": {
    "id": "3fZRMFbrwcYQ"
   },
   "source": [
    "# Information About Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "k98P_gpCwcYR",
   "metadata": {
    "id": "k98P_gpCwcYR"
   },
   "outputs": [],
   "source": [
    "def explore_dataframe(df):\n",
    "    print(\"-\"*30)\n",
    "    print(\"Overall Info of Columns :\")\n",
    "    print(df.info()) \n",
    "    print(\"-\"*30)\n",
    "    print(\"Quantity of Nans :\")\n",
    "    print(df.isna().sum()) \n",
    "    print(\"-\"*30)\n",
    "    print(\"Percentage of Nans :\")\n",
    "    print(df.isna().mean()*100) \n",
    "    print(\"-\"*30)\n",
    "    print(\"Variable Types :\")\n",
    "    print(df.dtypes)\n",
    "    print(\"-\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WZIlpAyJwcYT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WZIlpAyJwcYT",
    "outputId": "f19d82dc-3136-4275-8056-be91180d9418",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "explore_dataframe(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "leum3OBxwcYU",
   "metadata": {
    "id": "leum3OBxwcYU"
   },
   "source": [
    "# Visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DZFE-F0xwcYV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330
    },
    "id": "DZFE-F0xwcYV",
    "outputId": "3d56a4f7-6e59-47f6-945c-d5cc040fc312",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_target = df.groupby(\"target\").size()\n",
    "plt.pie(df_target.values, labels = [\"No Heart Disease\", \"With Heart Disease\"], autopct='%1.1f%%', radius = 1.5, textprops = {\"fontsize\" : 16}) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ubnf3XwcYW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "id": "a8ubnf3XwcYW",
    "outputId": "6bfe408e-8409-47e6-cb97-71a5d2813a27",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_sex = df.groupby([\"sex\",\"target\"]).size()\n",
    "plt.pie(df_sex.values, labels = [\"Female,No Heart Disease\", \"Female,With Heart Disease\", \"Male,No Heart Disease\", \"Male,With Heart Disease\"],autopct='%1.1f%%',radius = 1.5, textprops = {\"fontsize\" : 16})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bdf_fuwcYW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "id": "10bdf_fuwcYW",
    "outputId": "6eb6c880-ead9-4a2b-e05c-2770ea6d9668",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist([df[df.target==0].age, df[df.target==1].age], bins = 20, alpha = 0.5, label = [\"No Heart Disease\",\"With Heart Disease\"])\n",
    "plt.xlabel(\"age\")\n",
    "plt.ylabel(\"percentage\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dLxyvLx5wcYY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "id": "dLxyvLx5wcYY",
    "outputId": "4976c3a7-5957-4059-8f90-bd24e1bb235c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist([df[df.target==0].chol, df[df.target==1].chol], bins = 20, alpha = 0.5, label = [\"No Heart Disease\",\"With Heart Disease\"])\n",
    "plt.xlabel(\"chol\")\n",
    "plt.ylabel(\"percentage\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JVxH065YwcYZ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "id": "JVxH065YwcYZ",
    "outputId": "6d990c4e-2b72-4cb9-b84e-50807966cfea",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist([df[df.target==0].trestbps, df[df.target==1].trestbps], bins = 20, alpha = 0.5, label = [\"No Heart Disease\",\"With Heart Disease\"])\n",
    "plt.xlabel(\"trestbps\")\n",
    "plt.ylabel(\"percentage\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RkQyID3FwcYZ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "id": "RkQyID3FwcYZ",
    "outputId": "fbb8f241-ee3d-403f-8374-2d9edb1b1a17"
   },
   "outputs": [],
   "source": [
    "plt.hist([df[df.target==0].thalach, df[df.target==1].thalach], bins = 20, alpha = 0.5, label = [\"No Heart Disease\",\"With Heart Disease\"])\n",
    "plt.xlabel(\"thalach\")\n",
    "plt.ylabel(\"percentage\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_OYJfTN5wcYb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "id": "_OYJfTN5wcYb",
    "outputId": "80e1fcc8-86ce-4032-a050-f7f2d54bfe29",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.title('Heatmap Displaying the Relationship Betweennthe Features of the Data',fontsize=13)\n",
    "sns.heatmap(df.corr(), annot=True, cmap='Blues')\n",
    "hm = sns.heatmap(df.corr(), annot=True, cmap='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "X-Z_0bIDwcYd",
   "metadata": {
    "id": "X-Z_0bIDwcYd"
   },
   "source": [
    "## Feature Importance \n",
    "\n",
    "You can get the feature importance of each feature of your dataset by using the feature importance property of the model.\n",
    "Feature importance gives you a score for each feature of your data, the higher the score more important or relevant is the feature towards your output variable.\n",
    "Feature importance is an inbuilt class that comes with Tree Based Classifiers, we will be using Extra Tree Classifier for extracting the top 10 features for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "irf1lBsiwcYd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "id": "irf1lBsiwcYd",
    "outputId": "d7d48735-cad6-4667-aa67-ef56344c968b",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = ExtraTreesClassifier()\n",
    "model.fit(X,y)\n",
    "print(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n",
    "#plot graph of feature importances for better visualization\n",
    "feat_importances = pd.Series(model.feature_importances_ , index=X.columns)\n",
    "feat_importances.nlargest(10).plot(kind='barh')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mI7PBKurwcYd",
   "metadata": {
    "id": "mI7PBKurwcYd"
   },
   "outputs": [],
   "source": [
    "X = df[['ca','cp','thal','exang','thalach','oldpeak','age','age','slope','trestbps','chol']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xzEt2FEmdbVU",
   "metadata": {
    "id": "xzEt2FEmdbVU"
   },
   "outputs": [],
   "source": [
    "X = df.loc[:, df.columns != 'target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XnPzwVmJFaoA",
   "metadata": {
    "id": "XnPzwVmJFaoA"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scl=MinMaxScaler()\n",
    "X=scl.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scZ7D2JYwcYe",
   "metadata": {
    "id": "scZ7D2JYwcYe"
   },
   "outputs": [],
   "source": [
    "X = tf.keras.utils.normalize(X, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sotbm-eKwcYf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sotbm-eKwcYf",
    "outputId": "c14ec13d-1557-4f11-cb77-b97bb1e20d6c",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.30, random_state=0, shuffle=True)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4tmHNlwQwcYf",
   "metadata": {
    "id": "4tmHNlwQwcYf"
   },
   "source": [
    "### **HHO**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "E7A3-B3vKzLn",
   "metadata": {
    "id": "E7A3-B3vKzLn"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy\n",
    "import math\n",
    "\n",
    "\n",
    "def Levy(dim):\n",
    "    beta=1.5\n",
    "    sigma=(math.gamma(1+beta)*math.sin(math.pi*beta/2)/(math.gamma((1+beta)/2)*beta*2**((beta-1)/2)))**(1/beta) \n",
    "    u= 0.01*numpy.random.randn(dim)*sigma\n",
    "    v = numpy.random.randn(dim)\n",
    "    zz = numpy.power(numpy.absolute(v),(1/beta))\n",
    "    step = numpy.divide(u,zz)\n",
    "    return step\n",
    "\n",
    "\n",
    "\n",
    "def HHO(objf,lb,ub,dim,SearchAgents_no,Max_iter):\n",
    "\n",
    "    #dim=30\n",
    "    #SearchAgents_no=50\n",
    "    #lb=-100\n",
    "    #ub=100\n",
    "    #Max_iter=500\n",
    "        \n",
    "    \n",
    "    # initialize the location and Energy of the rabbit\n",
    "    Rabbit_Location=numpy.zeros(dim)\n",
    "    Rabbit_Energy=float(\"inf\")  #change this to -inf for maximization problems\n",
    "    \n",
    "    if not isinstance(lb, list):\n",
    "        lb = [lb for _ in range(dim)]\n",
    "        ub = [ub for _ in range(dim)]\n",
    "    lb = numpy.asarray(lb)\n",
    "    ub = numpy.asarray(ub)\n",
    "         \n",
    "    #Initialize the locations of Harris' hawks\n",
    "    X=numpy.zeros((SearchAgents_no, dim))\n",
    "    for i in range(SearchAgents_no):\n",
    "        for d in range(dim):\n",
    "            if(numpy.random.rand()>0.5):\n",
    "                X[i,d] = 1;\n",
    "            \n",
    "    fitR = math.inf; \n",
    "    fit  = numpy.zeros((N)); \n",
    "    Y    = numpy.zeros((dim)); \n",
    "    Z    = numpy.zeros((dim));\n",
    "    \n",
    "    curve =numpy.zeros(max_Iter);  \n",
    "    t = 1; \n",
    "    \n",
    "    \n",
    "    #---Iteration start-------------------------------------------------\n",
    "    while t <= max_Iter:\n",
    "      for i in range(SearchAgents_no):\n",
    "        fit[i] =objf(X[i,:]) #fun(feat,label,X(i,:),HO);\n",
    "        if fit[i] < fitR:\n",
    "          fitR = fit[i]\n",
    "          Xrb  = X[i,:]\n",
    "    \n",
    "      X_mu = numpy.mean(X,0);\n",
    "      for i in range(SearchAgents_no):\n",
    "        E0 = -1 + 2 * numpy.random.rand();\n",
    "        E  = 2 * E0 * (1 - (t / max_Iter)); \n",
    "        if abs(E) >= 1:\n",
    "          q = numpy.random.rand(); \n",
    "          if q >= 0.5:\n",
    "            k  = numpy.random.randint(1,SearchAgents_no);\n",
    "            r1 = numpy.random.rand();\n",
    "            r2 = numpy.random.rand();\n",
    "            for d in range(dim):\n",
    "              Xn = X[k,d] - r1 * abs(X[k,d] - 2 * r2 * X[i,d]);\n",
    "              S  = 1 / (1 + numpy.exp(-Xn));\n",
    "              if numpy.random.rand() < S:\n",
    "                X[i,d]= 1\n",
    "              else:\n",
    "                X[i,d] = 0\n",
    " \n",
    "          elif q < 0.5:\n",
    "            r3 = numpy.random.rand();\n",
    "            r4 = numpy.random.rand();\n",
    "            for d in range(dim):\n",
    "              Xn = (Xrb[d]- X_mu[d]) - r3 * (lb + r4 * (ub - lb));\n",
    "              S  = 1 / (1 + numpy.exp(-Xn));\n",
    "              if numpy.random.rand() < S[0]:\n",
    "                X[i,d] = 1;\n",
    "              else:\n",
    "                X[i,d] = 0;\n",
    "        elif abs(E) < 1:\n",
    "          J = 2 * (1 - numpy.random.rand());\n",
    "          r = numpy.random.rand();\n",
    "          if r >= 0.5  and  abs(E) >= 0.5:\n",
    "            for  d in range(dim):\n",
    "              DX = Xrb[d] - X[i,d];\n",
    "              Xn = DX - E * abs(J * Xrb[d]- X[i,d]);\n",
    "              S  = 1 / (1 + numpy.exp(-Xn));\n",
    "              if numpy.random.rand() < S:\n",
    "                 X[i,d]= 1\n",
    "              else:\n",
    "                X[i,d] = 0;\n",
    " \n",
    "          elif r >= 0.5  and  abs(E) < 0.5:\n",
    "            for  d in range(dim):\n",
    "              DX = Xrb[d] - X[i,d];\n",
    "              Xn = Xrb[d]- E * abs(DX);\n",
    "              S  = 1 / (1 + numpy.exp(-Xn));\n",
    "              if numpy.random.rand() < S:\n",
    "                 X[i,d]= 1\n",
    "              else:\n",
    "                 X[i,d]= 0\n",
    " \n",
    "          elif r < 0.5  and  abs(E) >= 0.5:\n",
    "            LF = Levy(dim); \n",
    "            for  d in range(dim):\n",
    "              Yn = Xrb[d] - E * abs(J * Xrb[d] - X[i,d]);\n",
    "              S  = 1 / (1 + numpy.exp(-Yn));\n",
    "              if numpy.random.rand() < S:\n",
    "                Y[d] = 1;\n",
    "              else:\n",
    "                Y[d] = 0;\n",
    " \n",
    "              Zn = Y[d] + numpy.random.rand() * LF[d];\n",
    "              S  = 1 / (1 + numpy.exp(-Zn));\n",
    "              if numpy.random.rand() < S:\n",
    "                Z[d]= 1;\n",
    "              else:\n",
    "                Z[d] = 0;\n",
    " \n",
    "            fitY =objf(Y)# fun(feat,label,Y,HO);\n",
    "            fitZ =objf(Z)# fun(feat,label,Z,HO);\n",
    "            if fitY <= fit[i]:\n",
    "              fit[i] = fitY; \n",
    "              X[i,:]= Y;\n",
    " \n",
    "            if fitZ <= fit[i]:\n",
    "              fit[i] = fitZ;\n",
    "              X[i,:]= Z;\n",
    " \n",
    "          elif r < 0.5  and  abs(E) < 0.5:\n",
    "            LF = Levy(dim); \n",
    "            for  d in range(dim):\n",
    "              Yn = Xrb[d] - E * abs(J * Xrb[d] - X_mu[d]);\n",
    "              S  = 1 / (1 + numpy.exp(-Yn));\n",
    "              if numpy.random.rand() < S:\n",
    "                Y[d]= 1;\n",
    "              else:\n",
    "                 Y[d] = 0;\n",
    " \n",
    "              Zn = Y[d] + numpy.random.rand() * LF[d];\n",
    "              S  = 1 / (1 + numpy.exp(-Zn));\n",
    "              if numpy.random.rand() < S:\n",
    "                Z[d] = 1;\n",
    "              else:\n",
    "                Z[d] = 0;\n",
    " \n",
    "            fitY =objf(Y)# fun(feat,label,Y,HO); \n",
    "            fitZ =objf(Z)# fun(feat,label,Z,HO);\n",
    "            if fitY <= fit[i]:\n",
    "              fit[i] = fitY; \n",
    "              X[i,:] = Y;\n",
    "        \n",
    "            if fitZ <= fit[i]:\n",
    "              fit[i] = fitZ; \n",
    "              X[i,:] = Z;\n",
    "  \n",
    "    \n",
    "  \n",
    "      curve[t-1] = fitR; \n",
    "      print('\\nIteration{0} Best (BHHO)= {1}'.format(t,curve[t-1]))\n",
    "      t = t + 1;\n",
    "\n",
    "        \n",
    "      \n",
    "\n",
    "\n",
    "    \n",
    "    return curve,Xrb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jjP__yA-9n11",
   "metadata": {
    "id": "jjP__yA-9n11"
   },
   "source": [
    "# **MLP Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VFTBVNlqx3kA",
   "metadata": {
    "id": "VFTBVNlqx3kA"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def Create_mlp_model(data,array_ner):\n",
    "  model = keras.Sequential([\n",
    "      layers.Dense(array_ner[0], activation='relu', input_shape=[data.shape[1]]),\n",
    "      layers.Dropout(0.5),\n",
    "      layers.BatchNormalization(),\n",
    "      layers.Dense(array_ner[1], activation='relu'), \n",
    "      layers.Dropout(0.5),\n",
    "      layers.BatchNormalization(),\n",
    "      layers.Dense(array_ner[2], activation='relu'), \n",
    "      #layers.Dropout(0.3),\n",
    "      layers.BatchNormalization(),\n",
    "      layers.Dense(1, activation='sigmoid'),\n",
    "  ])\n",
    "  model.compile(\n",
    "      optimizer='adam',\n",
    "      loss='binary_crossentropy',\n",
    "      metrics=['binary_accuracy'])\n",
    "  return model\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PF4eS0ziyeco",
   "metadata": {
    "id": "PF4eS0ziyeco"
   },
   "outputs": [],
   "source": [
    "def funcost(x):\n",
    "  x[x>=0.5]=1;\n",
    "  x[x<0.5]=0;\n",
    "  x=x.astype(int)\n",
    "  print(x)\n",
    "  array_ner=np.zeros(3,dtype=int)\n",
    "  array_ner[0]=int(''.join([str(elem) for elem in x[0:10]]),2)\n",
    "  array_ner[1]=int(''.join([str(elem) for elem in x[10:20]]),2)\n",
    "  array_ner[2]= int(''.join([str(elem) for elem in x[20:30]]),2)\n",
    "  print(array_ner)\n",
    "  \n",
    "  model=Create_mlp_model(data_test,array_ner)\n",
    "  model.fit(data_train,lbl_train,epochs=100,batch_size=20, verbose=0)\n",
    "  loss, accuracy = model.evaluate(data_test, lbl_test, verbose=0)\n",
    "\n",
    "  acc=accuracy\n",
    "  print(acc)\n",
    "  return 1/acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a_RUL_TzGUn",
   "metadata": {
    "id": "1a_RUL_TzGUn"
   },
   "outputs": [],
   "source": [
    "data_train, data_test, lbl_train, lbl_test = train_test_split( X, y, test_size=0.50, random_state=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OcSPQ2Z-26Wh",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OcSPQ2Z-26Wh",
    "outputId": "b042c33e-3878-4bbf-f383-771117deff93"
   },
   "outputs": [],
   "source": [
    "N= 10; \n",
    "max_Iter = 100;\n",
    "objf=funcost\n",
    "lb=0\n",
    "ub=1\n",
    "dim = 30;\n",
    "curve,best_sol=HHO(objf,lb,ub,dim,N,max_Iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "G_k4ah2V_xI0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G_k4ah2V_xI0",
    "outputId": "21a0c8a0-1f37-499a-a580-7126d6f3591a"
   },
   "outputs": [],
   "source": [
    "x=best_sol\n",
    "x=x.astype(int)\n",
    "array_ner=np.zeros(3,dtype=int)\n",
    "array_ner[0]=int(''.join([str(elem) for elem in x[0:10]]),2)\n",
    "array_ner[1]=int(''.join([str(elem) for elem in x[10:20]]),2)\n",
    "array_ner[2]= int(''.join([str(elem) for elem in x[20:30]]),2)\n",
    "print(array_ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8uoyThnt7VRQ",
   "metadata": {
    "id": "8uoyThnt7VRQ"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(array_ner[0], activation='relu', input_shape=[X_train.shape[1]]),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(array_ner[1], activation='relu'), \n",
    "    layers.Dropout(0.2),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(array_ner[2], activation='relu'), \n",
    "    layers.Dropout(0.2),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(1, activation='sigmoid'),\n",
    "])\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['binary_accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95NpB2IfwcYj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "95NpB2IfwcYj",
    "outputId": "65d3b49d-ed7f-4b18-b8cc-7101fcc4cd9c"
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    batch_size=10,\n",
    "    epochs=100,\n",
    "    verbose=1, # hide the output because we have so many epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Rdf76WJPwcYj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "Rdf76WJPwcYj",
    "outputId": "7f493993-df52-4c0d-973a-262863a5d58d"
   },
   "outputs": [],
   "source": [
    "history_df = pd.DataFrame(history.history)\n",
    "history_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aMz3eKgowcYk",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 550
    },
    "id": "aMz3eKgowcYk",
    "outputId": "56e5b381-7b00-48c6-a187-d0c4cf5c3a47"
   },
   "outputs": [],
   "source": [
    "history_df = pd.DataFrame(history.history)\n",
    "# Start the plot at epoch 5\n",
    "history_df.loc[:, ['loss', 'val_loss']].plot()\n",
    "history_df.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot()\n",
    "\n",
    "print((\"Best Validation Loss: {:0.4f}\" +\\\n",
    "      \"\\nBest Validation Accuracy: {:0.4f}\")\\\n",
    "      .format(history_df['val_loss'].min(), \n",
    "              history_df['val_binary_accuracy'].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fo4bzbhqwcYk",
   "metadata": {
    "id": "fo4bzbhqwcYk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ijFvcj-N0aY",
   "metadata": {
    "id": "3ijFvcj-N0aY"
   },
   "source": [
    "**CNN Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7GjuBrVDNzxn",
   "metadata": {
    "id": "7GjuBrVDNzxn"
   },
   "outputs": [],
   "source": [
    "def Create_CCN_model(array_ner):\n",
    "  \n",
    "  N,dim1=X_train.shape[0],X_train.shape[1]\n",
    "  input_layer=keras.layers.Input(shape=(dim1,1),name=\"input_layer\")\n",
    "  # array_ner[0]=100\n",
    "  # array_ner[1]=100\n",
    "  # array_ner[2]=100\n",
    "  array_ner=[270,810,760,300,0]\n",
    "  model=keras.models.Sequential([input_layer,\n",
    "                                keras.layers.Conv1D(filters=array_ner[0],kernel_size=(2),strides=(1),padding=\"same\",name=\"conv1\"),\n",
    "                                keras.layers.MaxPool1D(pool_size=(2),name=\"maxpool1\"),\n",
    "                                keras.layers.Dropout(0.2),\n",
    "                                keras.layers.BatchNormalization(),\n",
    "                                keras.layers.Conv1D(filters=array_ner[1],kernel_size=(2),strides=(1),padding=\"same\",name=\"conv2\"),\n",
    "                                keras.layers.MaxPool1D(pool_size=(2),name=\"maxpool2\"),\n",
    "                                keras.layers.Dropout(0.2),\n",
    "                                keras.layers.BatchNormalization(),\n",
    "                                keras.layers.Conv1D(filters=array_ner[2],kernel_size=(2),strides=(1),padding=\"same\",name=\"conv3\"),\n",
    "                                keras.layers.MaxPool1D(pool_size=(2),name=\"maxpool3\"),\n",
    "                                keras.layers.Dropout(0.2),\n",
    "                                keras.layers.BatchNormalization(),\n",
    "                                keras.layers.Flatten(),\n",
    "                                keras.layers.Dense(array_ner[3],activation=\"relu\"),\n",
    "                                keras.layers.Dense(1, activation='sigmoid',name=\"out_layer\")\n",
    "                                \n",
    "                                ]\n",
    "                                \n",
    "                              )\n",
    "  \n",
    "  model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['binary_accuracy'],)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42-A3YmQS4I_",
   "metadata": {
    "id": "42-A3YmQS4I_"
   },
   "outputs": [],
   "source": [
    "data_train_re=data_train.reshape(data_train.shape[0],data_train.shape[1],1)\n",
    "data_test_re=data_test.reshape(data_test.shape[0],data_test.shape[1],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c157U9JnSJs8",
   "metadata": {
    "id": "c157U9JnSJs8"
   },
   "outputs": [],
   "source": [
    "def funcost(x):\n",
    "  x[x>=0.5]=1;\n",
    "  x[x<0.5]=0;\n",
    "  x=x.astype(int)\n",
    "  print(x)\n",
    "  array_ner=np.zeros(3,dtype=int)\n",
    "  array_ner[0]=int(''.join([str(elem) for elem in x[0:7]]),2)\n",
    "  array_ner[1]=int(''.join([str(elem) for elem in x[7:14]]),2)\n",
    "  array_ner[2]= int(''.join([str(elem) for elem in x[14:21]]),2)\n",
    "  print(array_ner)\n",
    "  \n",
    "  model=Create_CCN_model(array_ner)\n",
    "  model.fit(data_train_re,lbl_train,epochs=100,batch_size=20, verbose=0)\n",
    "  loss, accuracy = model.evaluate(data_test_re, lbl_test, verbose=0)\n",
    "\n",
    "  acc=accuracy\n",
    "  print(acc)\n",
    "  return 1/acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WVkV7sZnSZpi",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 622
    },
    "id": "WVkV7sZnSZpi",
    "outputId": "b62340e4-5a1b-499b-9170-492463464334"
   },
   "outputs": [],
   "source": [
    "N= 2; \n",
    "max_Iter = 2;\n",
    "objf=funcost\n",
    "lb=0\n",
    "ub=1\n",
    "dim = 21;\n",
    "curve,best_sol=HHO(objf,lb,ub,dim,N,max_Iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5snFqesVV3Th",
   "metadata": {
    "id": "5snFqesVV3Th"
   },
   "outputs": [],
   "source": [
    "x=best_sol\n",
    "x=x.astype(int)\n",
    "\n",
    "array_ner=np.zeros(3,dtype=int)\n",
    "array_ner[0]=int(''.join([str(elem) for elem in x[0:10]]),2)\n",
    "array_ner[1]=int(''.join([str(elem) for elem in x[10:20]]),2)\n",
    "array_ner[2]= int(''.join([str(elem) for elem in x[20:30]]),2)\n",
    "print(array_ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GB1gyh7TW46-",
   "metadata": {
    "id": "GB1gyh7TW46-"
   },
   "outputs": [],
   "source": [
    "X_train_re=X_train.reshape(X_train.shape[0],X_train.shape[1],1)\n",
    "X_test_re=X_test.reshape(X_test.shape[0],X_test.shape[1],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FZ7ONaJPWZax",
   "metadata": {
    "id": "FZ7ONaJPWZax"
   },
   "outputs": [],
   "source": [
    "model=Create_CCN_model(array_ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tIq4dj-eW2w5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tIq4dj-eW2w5",
    "outputId": "a85ea301-fbd9-49cc-8c8a-d438f7917e18"
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train_re, y_train,\n",
    "    validation_data=(X_test_re, y_test),\n",
    "    batch_size=10,\n",
    "    epochs=100,\n",
    "    verbose=1, # hide the output because we have so many epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Buf13NwzXNSu",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "Buf13NwzXNSu",
    "outputId": "51537abf-a2d9-4f50-bfdd-f358a2e3585f"
   },
   "outputs": [],
   "source": [
    "history_df = pd.DataFrame(history.history)\n",
    "history_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59YfqiiqXZdF",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 550
    },
    "id": "59YfqiiqXZdF",
    "outputId": "b90c289a-8012-47c9-f5bf-d2c042f272d5"
   },
   "outputs": [],
   "source": [
    "history_df = pd.DataFrame(history.history)\n",
    "# Start the plot at epoch 5\n",
    "history_df.loc[:, ['loss', 'val_loss']].plot()\n",
    "history_df.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot()\n",
    "\n",
    "print((\"Best Validation Loss: {:0.4f}\" +\\\n",
    "      \"\\nBest Validation Accuracy: {:0.4f}\")\\\n",
    "      .format(history_df['val_loss'].min(), \n",
    "              history_df['val_binary_accuracy'].max()))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "4tmHNlwQwcYf",
    "jjP__yA-9n11"
   ],
   "name": "heart_CNN_MLP_HHO.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
